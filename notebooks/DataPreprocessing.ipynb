{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502f84b0-af01-4a46-b342-192f55782b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "#define one_hot encoding function\n",
    "def one_hot(seq):\n",
    "    \"\"\"Convert RNA string to (4x201) one-hot encoding, float32 array.\"\"\"\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'T': 3, 'N': 4}\n",
    "    arr = np.zeros((4, len(seq)), dtype=np.float32)\n",
    "    for i, nt in enumerate(seq.upper()):\n",
    "        idx = mapping.get(nt, 4)\n",
    "        if idx < 4:\n",
    "            arr[idx,i] = 1.0\n",
    "    return arr\n",
    "\n",
    "#define dataset object (expecting pair of pos and negative for train, val, and test respectively)\n",
    "\n",
    "class PasDataset(Dataset):\n",
    "    def __init__(self, fasta_paths, labels):\n",
    "        \"\"\"\n",
    "        fasta_paths: list of FASTA file paths\n",
    "        labels: corresponding labels [1,0]\n",
    "        \"\"\"\n",
    "        self.seqs = []\n",
    "        self.ys = []\n",
    "        \n",
    "        #loop through fasta paths and associated labels -> parse each fasta for 201nt segments -> onehot encode and append to seqs.\n",
    "        for path, y in zip(fasta_paths, labels):\n",
    "            for rec in SeqIO.parse(path, \"fasta\"):\n",
    "                seq = str(rec.seq)\n",
    "                if len(seq) != 201:\n",
    "                    print('skipping sequence that is not 201bp')\n",
    "                    continue\n",
    "                self.seqs.append(one_hot(str(rec.seq)))\n",
    "                self.ys.append(y)\n",
    "                                 \n",
    "        self.seqs = torch.tensor(self.seqs)\n",
    "        self.ys = torch.tensor(self.ys, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.seqs[i], self.ys[i]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf5120f-d339-4b5e-b787-2b223dfe8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "pos = list(SeqIO.parse('../data/pos_201_hg19.fa', 'fasta'))\n",
    "neg = list(SeqIO.parse('../data/neg_201_hg19.fa', 'fasta'))\n",
    "random.seed(42)\n",
    "random.shuffle(pos)\n",
    "random.shuffle(neg)\n",
    "\n",
    "def splits(lst, train_frac = 0.7, val_frac = 0.15):\n",
    "    #splits the list of sequences based on index corresponding to set size. \n",
    "    n = len(lst)\n",
    "    i_train = int(train_frac * n)\n",
    "    i_val = i_train + int(val_frac * n)\n",
    "    return lst[:i_train], lst[i_train:i_val], lst[i_val:]\n",
    "\n",
    "pos_train, pos_val, pos_test = splits(pos)\n",
    "neg_train, neg_val, neg_test = splits(neg)\n",
    "\n",
    "#create separate fasta files for each set\n",
    "from pathlib import Path\n",
    "\n",
    "def write_split(records, path):\n",
    "    Path(path).parent.mkdir(exist_ok = True)\n",
    "    SeqIO.write(records, path, 'fasta')\n",
    "\n",
    "write_split(pos_train, '../data/processed/pos_201_train.fa')\n",
    "write_split(pos_val,   '../data/processed/pos_201_val.fa')\n",
    "write_split(pos_test,  '../data/processed/pos_201_test.fa')\n",
    "\n",
    "write_split(neg_train, '../data/processed/neg_201_train.fa')\n",
    "write_split(neg_val,   '../data/processed/neg_201_val.fa')\n",
    "write_split(neg_test,  '../data/processed/neg_201_test.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "902e53ab-4b19-4d03-a4eb-4b700a43f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POLYNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size = 8, padding = 'same')\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size = 6, padding='same')\n",
    "        self.gmp = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.gmp(x).squeeze(-1)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb2f9361-bd91-48f2-8a77-82572260a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            preds = model(xb)\n",
    "            ys.extend(yb.cpu().numpy())\n",
    "            ps.extend(preds.cpu().numpy())\n",
    "    return roc_auc_score(ys, ps), average_precision_score(ys, ps)\n",
    "\n",
    "#training loop\n",
    "def train_and_evaluate(\n",
    "    train_files, val_files, test_files,\n",
    "    batch_size = 64, lr = 1e-3, epochs = 10\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Datasets and loaders\n",
    "    train_ds = PasDataset(train_files, [1,0])\n",
    "    val_ds = PasDataset(val_files, [1,0])\n",
    "    test_ds = PasDataset(test_files, [1,0])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle = True, num_workers = 4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle = False, num_workers = 4)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle = False, num_workers = 4)\n",
    "\n",
    "    model = POLYNET().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    lossf = nn.BCELoss()\n",
    "\n",
    "    #training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            opt.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = lossf(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        #Validation\n",
    "        val_auc, val_auprc = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch:2d} | train_loss {train_loss:.4f} | \"\n",
    "        f\"val_AUROC {val_auc:.4f} | val_AUPRC {val_auprc:.4f}\")\n",
    "\n",
    "    # Final test\n",
    "    test_auc, test_auprc = evaluate(model, test_loader, device)\n",
    "    print(f\"Test set - AUROC: {test_auc:.4f}, AUPRC: {test_auprc:.4f}\")\n",
    "\n",
    "    #Save model\n",
    "    torch.save(model.state_dict(), \"../models/POLYNET.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a80106cd-2387-4cec-be23-968f1b9f7192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping sequence that is not 201bp\n",
      "Epoch  1 | train_loss 0.5052 | val_AUROC 0.8557 | val_AUPRC 0.8469\n",
      "Epoch  2 | train_loss 0.4704 | val_AUROC 0.8615 | val_AUPRC 0.8531\n",
      "Epoch  3 | train_loss 0.4610 | val_AUROC 0.8636 | val_AUPRC 0.8551\n",
      "Epoch  4 | train_loss 0.4551 | val_AUROC 0.8657 | val_AUPRC 0.8574\n",
      "Epoch  5 | train_loss 0.4508 | val_AUROC 0.8664 | val_AUPRC 0.8580\n",
      "Epoch  6 | train_loss 0.4477 | val_AUROC 0.8673 | val_AUPRC 0.8590\n",
      "Epoch  7 | train_loss 0.4453 | val_AUROC 0.8662 | val_AUPRC 0.8585\n",
      "Epoch  8 | train_loss 0.4429 | val_AUROC 0.8675 | val_AUPRC 0.8596\n",
      "Epoch  9 | train_loss 0.4417 | val_AUROC 0.8678 | val_AUPRC 0.8597\n",
      "Epoch 10 | train_loss 0.4399 | val_AUROC 0.8671 | val_AUPRC 0.8592\n",
      "Test set - AUROC: 0.8662, AUPRC: 0.8586\n"
     ]
    }
   ],
   "source": [
    "#training and evaluation\n",
    "\n",
    "train_files = [\"../data/processed/pos_201_train.fa\", \"../data/processed/neg_201_train.fa\"]\n",
    "val_files   = [\"../data/processed/pos_201_val.fa\",   \"../data/processed/neg_201_val.fa\"]\n",
    "test_files  = [\"../data/processed/pos_201_test.fa\",  \"../data/processed/neg_201_test.fa\"]\n",
    "\n",
    "train_and_evaluate(\n",
    "    train_files, val_files, test_files,\n",
    "    batch_size = 64, lr = 1e-3, epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfab2cf-7422-4843-a948-515241c39684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
